
# CPU Pipelining

Module: Computer Systems Organization

---

## 1.3.0 The Microarchitectural Concept

**Instruction Pipelining** is a technique that increases CPU throughput (instructions completed per second) by overlapping the execution of multiple instructions. It transforms the CPU from a sequential processor into a parallel assembly line.

### Key Distinction: Latency vs. Throughput

- **Latency:** The time to complete _one_ specific instruction (does not improve with pipelining; often worsens slightly due to stage overhead).
    
- **Throughput:** The number of instructions completing per unit of time (drastically improves).
    

> **Analogy:** Doing laundry.
> 
> - **Sequential:** Wash Load A ‚Üí Dry Load A ‚Üí Fold Load A ‚Üí Wash Load B...
>     
> - **Pipelined:** Wash Load C / Dry Load B / Fold Load A (All happening simultaneously).
>     

---

## 1.3.1 The Classic 5-Stage RISC Pipeline

Derived from the Patterson & Hennessy MIPS/RISC-V model, this is the standard academic baseline.

### The Stages

1. **IF (Fetch):** Read instruction from memory at address PC.
    
2. **ID (Decode):** Read registers and decode opcode.
    
3. **EX (Execute):** ALU calculates result or memory address.
    
4. **MEM (Memory):** Read/Write to data cache.
    
5. **WB (Write-Back):** Write result into register file.
    

### Visualizing the Stagger (The "Waterfall")

In an ideal pipeline, one instruction enters and one retires every single clock cycle.

|**Cycle**|**1**|**2**|**3**|**4**|**5**|**6**|**7**|
|---|---|---|---|---|---|---|---|
|**Instr 1**|**IF**|ID|EX|MEM|WB|||
|**Instr 2**||**IF**|ID|EX|MEM|WB||
|**Instr 3**|||**IF**|ID|EX|MEM|WB|

---

## 1.3.2 Quantitative Performance

### The Speedup Equation

In a perfect world, the speedup from pipelining equals the number of stages. In reality, **stalls** reduce this.

$$\text{Speedup} = \frac{\text{Pipeline Depth}}{1 + \text{Stall Cycles per Instruction}}$$

- **Ideal CPI (Cycles Per Instruction):** $\approx 1$
    
- **Real World:** Stalls (Hazards) push CPI $> 1$.
    

### Depth vs. Frequency

Architects face a trade-off regarding **Pipeline Depth**:

- **Shallow (e.g., 5 stages):** Low frequency, but low penalty on branch misprediction.
    
- **Deep (e.g., 20+ stages, "Super-pipelining"):** Very high frequency (less work per stage), but severe penalty on misprediction.
    
    - _Historical Note:_ The Intel Pentium 4 (NetBurst) failed partly because its 31-stage pipeline made branch mistakes too costly to recover from.
        

---

## 1.3.3 Pipeline Hazards

A **Hazard** prevents the next instruction from executing in the following cycle.

### 1. Structural Hazards (Resource Conflict)

Hardware cannot support the combination of instructions.

- _Example:_ Instruction A needs the ALU to add; Instruction B needs the ALU to calculate a memory address. If there is only one ALU, one must wait.
    
- _Solution:_ Add more hardware (e.g., separate Data and Instruction Caches; multiple ALUs).
    

### 2. Data Hazards (Dependencies)

An instruction depends on the result of a previous instruction that hasn't finished yet.

- **RAW (Read After Write):** The most common hazard.
    
    Code snippet
    
    ```
    add x1, x2, x3   ; Writes to x1
    sub x4, x1, x5   ; Reads x1 (Wait! x1 isn't ready yet!)
    ```
    
- **Solution 1: Stalling (Bubbles):** Pause the dependent instruction.
    
- **Solution 2: Forwarding (Bypassing):** Hardware routes the ALU result from the previous instruction _directly_ to the input of the next, skipping the Write-Back stage.
    

### 3. Control Hazards (Branching)

The pipeline needs to fetch the _next_ instruction, but the current instruction is a branch (`if x > y`), and we don't know the result yet.

- _The Problem:_ Which address do we fetch next?
    
- _The Cost:_ If we guess wrong, we must **Flush** the pipeline (throw away all work in progress).
    
- _Solution:_ **Branch Prediction** (Guessing the outcome).
    

---

## 1.3.4 Visualizing a Stall (The "Bubble")

A bubble is a NOP (No-Operation) injected into the pipeline to buy time.

**Scenario:** Data Hazard without Forwarding.

|**Cycle**|**1**|**2**|**3**|**4**|**5**|**6**|
|---|---|---|---|---|---|---|
|**Instr 1 (Prod)**|IF|ID|**EX**|MEM|WB||
|**Instr 2 (Cons)**||IF|ID|**(Stall)**|**EX**|MEM|

_Note how Instruction 2 is delayed by one cycle (the bubble) waiting for data._

---

## 1.3.5 Advanced Concepts

### Superscalar Architecture

If Pipelining is an assembly line, **Superscalar** is having multiple assembly lines running in parallel.

- **Scalar:** Fetch 1, Decode 1, Execute 1.
    
- **Superscalar:** Fetch 4, Decode 4, Execute 4.
    
- _Requirement:_ Complex logic to check for hazards across _all_ simultaneous instructions.
    

### OS-Level Implications

1. **Context Switching Overhead:** Deep pipelines act as "inertia." Stopping a process requires draining the pipeline and saving state. Restarting it requires "filling" the empty pipeline.
    
2. **Hyper-Threading (SMT):** Uses one physical pipeline to run two software threads. This fills "bubbles" in one thread with useful work from the other, increasing utilization.
    

---

## üìù Problem Set

### 1. Hazard Identification

**Given Code:**

Code snippet

```
1. lw  x1, 0(x5)    ; Load x1 from memory
2. add x2, x1, x3   ; Use x1
3. sw  x2, 0(x5)    ; Store x2
```

Task: Identify the hazard between line 1 and 2.

Answer: RAW (Read After Write) Data Hazard. The add instruction needs the value of x1, but lw (Load Word) takes time to retrieve it from memory. Even with forwarding, a load-use hazard usually requires 1 stall cycle.

### 2. The Cost of Depth

Scenario: A CPU has a 20-stage pipeline. A branch decision is made at Stage 10.

Question: If the branch predictor guesses wrong, how many cycles are wasted?

Answer: 10 Cycles. All instructions fetched into stages 1-9 are wrong and must be flushed.

### 3. System Design

Scenario: You are designing a CPU for a device that runs simple, linear code with very few branches.

Decision: Would you choose a Deep Pipeline (20 stages) or a Shallow Pipeline (5 stages)?

Answer: Deep Pipeline. Since there are few branches, the risk of misprediction penalties is low. You can leverage the deep pipeline to achieve a very high Clock Frequency and maximize throughput.

---

## üîó Connection Links

- **‚óÄÔ∏è Previous (1.2):** ALU/FPU (The units doing the work in the 'EX' stage).
    
- **‚ñ∂Ô∏è Next (1.4):** **Out-of-Order Execution**. (How modern CPUs solve the stalling problem by rearranging the instruction stream).
    